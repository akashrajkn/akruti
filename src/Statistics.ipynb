{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "from models import WordEncoder, Attention, TagEmbedding, WordDecoder, MSVED, KumaMSD\n",
    "from dataset import MorphologyDatasetTask3, Vocabulary\n",
    "\n",
    "from kumaraswamy import Kumaraswamy\n",
    "from hard_kumaraswamy import StretchedAndRectifiedDistribution as HardKumaraswamy\n",
    "\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'turkish'\n",
    "f_task   = 'task1'\n",
    "f_type   = 'train'\n",
    "\n",
    "filepath = '../data/files/{}-{}-{}'.format(language, f_task, f_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 12336\n"
     ]
    }
   ],
   "source": [
    "with open(filepath, 'r') as f:\n",
    "    source = f.read()\n",
    "\n",
    "lines = source.strip().split('\\n')\n",
    "\n",
    "print('Total lines: {}'.format(len(lines)))\n",
    "\n",
    "unique_x_s = defaultdict(int)\n",
    "unique_x_t = defaultdict(int)\n",
    "unique_x   = defaultdict(int)\n",
    "unique_msd = defaultdict(int)\n",
    "\n",
    "for line in lines:\n",
    "    words = line.strip().split('\\t')\n",
    "    msds  = words[1].strip().split(',')\n",
    "\n",
    "    unique_x_s[words[0]] += 1\n",
    "    unique_x_t[words[2]] += 1\n",
    "    unique_x[words[0]]   += 1\n",
    "    unique_x[words[2]]   += 1\n",
    "\n",
    "    for msd in msds:\n",
    "        unique_msd[msd]  += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique source str: 2353\n",
      "Unique target str: 12005\n",
      "Unique words     : 14231\n",
      "Unique MSDs      : 31\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Unique source str: {}'.format(len(unique_x_s)))\n",
    "print('Unique target str: {}'.format(len(unique_x_t)))\n",
    "print('Unique words     : {}'.format(len(unique_x)))\n",
    "print('Unique MSDs      : {}'.format(len(unique_msd)))\n",
    "print('\\n')\n",
    "# print(unique_x_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alamet': 13, 'arzu': 13, 'bakma': 16, 'bohça': 13, 'dernek': 13, 'düşman': 14, 'elbise': 13, 'kas': 14, 'keçi': 15, 'kibutz': 13, 'lastik': 13, 'müellif': 13, 'nedime': 14, 'sağ': 13, 'yeterlilik': 13}\n"
     ]
    }
   ],
   "source": [
    "reduced_d = {k: v for k, v in unique_x_s.items() if v > 12}\n",
    "\n",
    "print(reduced_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'r') as f:\n",
    "    source = f.read()\n",
    "\n",
    "lines  = source.strip().split('\\n')\n",
    "keys   = reduced_d.keys()\n",
    "output = []\n",
    "\n",
    "for line in lines:\n",
    "    words = line.strip().split('\\t')\n",
    "\n",
    "    if words[0] in keys:\n",
    "        output.append(line)\n",
    "        \n",
    "with open('../data/{}_stem_greater_12'.format(language), 'w+') as f:\n",
    "    f.write('\\n'.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
